{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machado_Assis_GRU.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "cZcotldsd3-R",
        "yWHqTp9RiKIv",
        "G6dPmGsVjmqo"
      ],
      "toc_visible": true,
      "mount_file_id": "https://github.com/CeHaga/machado-gru/blob/main/Machado_Assis_GRU.ipynb",
      "authorship_tag": "ABX9TyPmFnjiBsJUZ+crX9spxB90",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CeHaga/machado-gru/blob/main/Machado_Assis_GRU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEw2Cz0xdkTy"
      },
      "source": [
        "#Machado de Assis GRU\n",
        "This notebook trains a Gated Recurrent Unit using texts from Brazilian author Machado de Assis.\n",
        "\n",
        "It's based on this TensorFlow tutorial for Shakespare works: https://www.tensorflow.org/tutorials/text/text_generation?hl=en"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZcotldsd3-R"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCpH9ZGudj2U"
      },
      "source": [
        "# Basic packages\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_5GgwWJeItJ",
        "outputId": "5c9188fc-8d35-409f-9280-378f0d8addab"
      },
      "source": [
        "# Read poetry file\n",
        "path = '/content/drive/MyDrive/Machado_LSTM/poesias.txt'\n",
        "text = open(path).read()\n",
        "print('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 650723 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsaIjSsKh03S",
        "outputId": "1b009e9b-9ef5-4891-eb77-837519f98ed9"
      },
      "source": [
        "# Get unique characters for vocabulary\n",
        "vocab = sorted(set(text))\n",
        "print('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "120 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgFOOJhK5G9i"
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = '/content/drive/MyDrive/Machado_LSTM/training_checkpoints'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWHqTp9RiKIv"
      },
      "source": [
        "## Text Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6dPmGsVjmqo"
      },
      "source": [
        "### Mapping\n",
        "Create a mapping from unique characters to indices and *vice versa*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3MYopKRiMpe",
        "outputId": "affd82cf-1178-4ce2-c914-9a7c12e331a7"
      },
      "source": [
        "# Represent each char as an index\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "\n",
        "print('{')\n",
        "for char,_ in zip(char2idx, range(8)):\n",
        "  print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  '\\n':   0,\n",
            "  ' ' :   1,\n",
            "  '!' :   2,\n",
            "  '\"' :   3,\n",
            "  \"'\" :   4,\n",
            "  '(' :   5,\n",
            "  ')' :   6,\n",
            "  '*' :   7,\n",
            "  ...\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IC9qF233j3uQ",
        "outputId": "4a9a7692-7ee5-4a10-92b9-c00a8e354743"
      },
      "source": [
        "# Represent each index as a char (Opposite of before)\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "print('{')\n",
        "for i in range(8):\n",
        "  print('  {:d}: {:s},'.format(i, repr(idx2char[i])))\n",
        "print('  ...\\n}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  0: '\\n',\n",
            "  1: ' ',\n",
            "  2: '!',\n",
            "  3: '\"',\n",
            "  4: \"'\",\n",
            "  5: '(',\n",
            "  6: ')',\n",
            "  7: '*',\n",
            "  ...\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlT9wLcEj34y",
        "outputId": "f89ed9e7-6d22-4978-d3e2-ce0129fd0bd5"
      },
      "source": [
        "# Get input data as int representation\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "\n",
        "print('{} ---- characters mapped to int ---- > {}'.format(repr(text[:10]), text_as_int[:10]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'O ALMADA\\nP' ---- characters mapped to int ---- > [39  1 25 36 37 25 28 25  0 40]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5tZ1srOm3em"
      },
      "source": [
        "## Model Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PW3Sfg8nbCJ"
      },
      "source": [
        "### Create Input Sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjoYWOGUnc_A",
        "outputId": "4db96ced-3040-4b76-8c87-ff177a22326b"
      },
      "source": [
        "# Set size of sequences\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Convert vector to indices stream\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "  print(idx2char[i.numpy()])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "O\n",
            " \n",
            "A\n",
            "L\n",
            "M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NTHz_xcpIQX",
        "outputId": "edd7376c-7782-4464-c9c8-ce46ec25a75b"
      },
      "source": [
        "# Create batches\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'O ALMADA\\nPoema herói-cômico em 8 cantos\\n(Fragmentos)\\nADVERTÊNCIA\\nO assunto deste poema é rigorosament'\n",
            "'e histórico. Em 1659, era prelado administrador do Rio\\nde Janeiro o Dr. Manuel de Sousa Almada, presb'\n",
            "'ítero do hábito de São Pedro. Um tabelião, por\\nnome Sebastião Ferreira Freire, foi vítima de uma assu'\n",
            "'ada, em certa noite, na ocasião em que\\nse recolhia para casa. Queixando-se ao ouvidor-geral Pedro de '\n",
            "'Mustre Portugal, abriu este\\ndevassa, vindo a saber-se que eram autores do delito alguns fâmulos do pr'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BU7QRZUMprz6",
        "outputId": "4e48c379-6494-41c3-c076-ef95ac58fb43"
      },
      "source": [
        "# Create a input vector and a target vector\n",
        "\n",
        "def split_input_target(chunk):\n",
        "  input_text = chunk[:-1]\n",
        "  target_text = chunk[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "for input_example, target_example in  dataset.take(1):\n",
        "  print('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  'O ALMADA\\nPoema herói-cômico em 8 cantos\\n(Fragmentos)\\nADVERTÊNCIA\\nO assunto deste poema é rigorosamen'\n",
            "Target data: ' ALMADA\\nPoema herói-cômico em 8 cantos\\n(Fragmentos)\\nADVERTÊNCIA\\nO assunto deste poema é rigorosament'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lz-MLwnMt12Z"
      },
      "source": [
        "### Create Batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYx6reoyt47u"
      },
      "source": [
        "# Set batch size\n",
        "batch_size = 64\n",
        "buffer_size = 10000\n",
        "\n",
        "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLkkxXc7uZ4k"
      },
      "source": [
        "### Create Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4sTQLMMudkp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faeaa3dd-f990-4d18-bfc8-4dfbf69640a4"
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        # Meaning\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                                  batch_input_shape=[batch_size, None]),\n",
        "        # Context\n",
        "        tf.keras.layers.GRU(rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            stateful=True,\n",
        "                            recurrent_initializer='glorot_uniform'),\n",
        "        # Output\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           30720     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 120)           123000    \n",
            "=================================================================\n",
            "Total params: 4,092,024\n",
            "Trainable params: 4,092,024\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsGh_YUzx8L3"
      },
      "source": [
        "### Check a Prediction Before "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JhzN_rXwU2y",
        "outputId": "3e457436-d6e2-487f-f422-ca170d308705"
      },
      "source": [
        "example_batch_predictions = ''\n",
        "\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "\n",
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
        "\n",
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print(\"\\nNext Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            " ' um irmão o afeto casto,\\nTanto pudor nessa criatura havia!\\nNem um som despertava em nossos lábios;\\nE'\n",
            "\n",
            "Next Char Predictions: \n",
            " \"2ÍiNêÇ*AMÚ’Ô!ñ°lTfvVm:Â2ôêé´\\n_ÕêÃ”ê6!zÓYevém;mMnª5P”5c/6:uáDñ11kÁ0O)díî'WmùÂvU‘mºjo9mEXi)Íb Vm(ñàñûq\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzASJ4Zezr6j"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxoTQ_Jjztso"
      },
      "source": [
        "# Compile model\n",
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDZ7OPKx0Fmj"
      },
      "source": [
        "# Configure checkpoints\n",
        "\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZYNXNMK0nZ3"
      },
      "source": [
        "# Train\n",
        "epochs = 50\n",
        "\n",
        "history = model.fit(dataset, initial_epoch=50, epochs=epochs, callbacks=[checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jz6Bg64M2cGS"
      },
      "source": [
        "## Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4V1Q8T9C3KRh"
      },
      "source": [
        "# Set batch size to 1 for easier prediction\n",
        "tf.train.latest_checkpoint(checkpoint_dir)\n",
        "\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhygocIa3XBt"
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 350\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperature results in more predictable text.\n",
        "  # Higher temperature results in more surprising text.\n",
        "  temperature = 1\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    # remove the batch dimension\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "    # using a categorical distribution to predict the character returned by the model\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "    # Pass the predicted character as the next input to the model\n",
        "    # along with the previous hidden state\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZmf7YgL4kKK",
        "outputId": "6a539001-374f-4db5-b64a-6e004d542bf1"
      },
      "source": [
        "print(generate_text(model, start_string=u\"Vida \"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vida flor dos Cinges voando, com pequenina,\n",
            "Leva o coração terrenho,\n",
            "Euscrivão, quero, excelso\n",
            "Recomenda\n",
            "Para a espécie humilde\n",
            "De sérias rapira.\n",
            "Isto que uma sereno\n",
            "Leitor, tiração de menina e de outro nosso,\n",
            "Adultar em si o sol da minha pós; Não vês\n",
            "custam modernos à mantira,\n",
            "Potira acaso uma flor derramava\n",
            "Era lhe creio.\n",
            "Fastou-lhe às margens do vigá\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}